{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## Importing data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom itertools import groupby\nfrom sklearn.model_selection import train_test_split\nfrom pandas.api.types import is_datetime64_ns_dtype\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport lightgbm as lgb\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom joblib import Parallel, delayed\nimport gc\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom metric import score # Import event detection ap score function\n\n# These are variables to be used by the score function\ncolumn_names = {\n    'series_id_column_name': 'series_id',\n    'time_column_name': 'step',\n    'event_column_name': 'event',\n    'score_column_name': 'score',\n}\n\ntolerances = {\n    'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360], \n    'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:25.194881Z","iopub.execute_input":"2023-10-17T23:11:25.195882Z","iopub.status.idle":"2023-10-17T23:11:29.864937Z","shell.execute_reply.started":"2023-10-17T23:11:25.195851Z","shell.execute_reply":"2023-10-17T23:11:29.864082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \n    \"\"\" \n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.        \n    \"\"\"\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object and not is_datetime64_ns_dtype(df[col]) and not 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int32)  \n            else:\n                df[col] = df[col].astype(np.float16)\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:29.866599Z","iopub.execute_input":"2023-10-17T23:11:29.867184Z","iopub.status.idle":"2023-10-17T23:11:29.878214Z","shell.execute_reply.started":"2023-10-17T23:11:29.867156Z","shell.execute_reply":"2023-10-17T23:11:29.875939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef feat_eng(df):\n    \n    df['series_id'] = df['series_id'].astype('category')\n    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n    df['hour'] = df[\"timestamp\"].dt.hour\n    # df['minute'] = df[\"timestamp\"].dt.minute\n    \n    df.sort_values(['timestamp'], inplace=True)\n    df.set_index('timestamp', inplace=True)\n    \n    df['lids'] = np.maximum(0., df['enmo'] - 0.02)\n    df['lids'] = df['lids'].rolling(f'{120*5}s', center=True, min_periods=1).agg('sum')\n    df['lids'] = 100 / (df['lids'] + 1)\n    df['lids'] = df['lids'].rolling(f'{360*5}s', center=True, min_periods=1).agg('mean').astype(np.float32)\n    \n    df[\"enmo\"] = (df[\"enmo\"]*1000).astype(np.int16)\n    df[\"anglez\"] = df[\"anglez\"].astype(np.int16)\n    df[\"anglezdiffabs\"] = df[\"anglez\"].diff().abs().astype(np.float32)\n    \n    for col in ['enmo', 'anglez', 'anglezdiffabs']:\n\n            # periods in seconds        \n        periods = [60, 300, 720, 1080, 1800, 3600, 5400, 7200, 14400, 21600]\n\n        for n in periods:\n\n            rol_args = {'window':f'{n+5}s', 'min_periods':12, 'center':True}\n\n            for agg in ['median', 'mean', 'max', 'min', 'var']:\n                df[f'{col}_{agg}_{n}'] = df[col].rolling(**rol_args).agg(agg).astype(np.float32).values\n                gc.collect()\n\n            if n == max(periods):\n                df[f'{col}_mad_{n}'] = (df[col] - df[f'{col}_median_{n}']).abs().rolling(**rol_args).median().astype(np.float32)\n\n            df[f'{col}_amplit_{n}'] = df[f'{col}_max_{n}']-df[f'{col}_min_{n}']\n            df[f'{col}_amplit_{n}_min'] = df[f'{col}_amplit_{n}'].rolling(**rol_args).min().astype(np.float32).values\n\n#             if col in ['enmo', 'anglez']:\n            df[f'{col}_diff_{n}_max'] = df[f'{col}_max_{n}'].diff().abs().rolling(**rol_args).max().astype(np.float32)\n            df[f'{col}_diff_{n}_mean'] = df[f'{col}_max_{n}'].diff().abs().rolling(**rol_args).mean().astype(np.float32)\n\n            gc.collect()\n\n    df.reset_index(inplace=True)\n    df.dropna(inplace=True)\n    df = df.iloc[::120]\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:29.880063Z","iopub.execute_input":"2023-10-17T23:11:29.881085Z","iopub.status.idle":"2023-10-17T23:11:29.902954Z","shell.execute_reply.started":"2023-10-17T23:11:29.88104Z","shell.execute_reply":"2023-10-17T23:11:29.90141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = '/kaggle/input/zzzs-lightweight-training-dataset-target/Zzzs_train.parquet'\n\ndef feat_eng_by_id(idx):\n    \n    from warnings import simplefilter \n    simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n    \n    df  = pd.read_parquet(file, filters=[('series_id','=',idx)])\n    df['awake'] = df['awake'].astype(np.int8)\n    df = feat_eng(df)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:29.905999Z","iopub.execute_input":"2023-10-17T23:11:29.906445Z","iopub.status.idle":"2023-10-17T23:11:29.920656Z","shell.execute_reply.started":"2023-10-17T23:11:29.906413Z","shell.execute_reply":"2023-10-17T23:11:29.919821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and validating ","metadata":{}},{"cell_type":"code","source":"DEV = False\n\nseries_id  = pd.read_parquet(file, columns=['series_id'])\nseries_id = series_id.series_id.unique()\n\nprint(len(series_id))\n\nif DEV:\n    series_id = series_id[::10]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:29.92294Z","iopub.execute_input":"2023-10-17T23:11:29.923689Z","iopub.status.idle":"2023-10-17T23:11:32.389552Z","shell.execute_reply.started":"2023-10-17T23:11:29.923638Z","shell.execute_reply":"2023-10-17T23:11:32.388375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weird_series = ['31011ade7c0a', 'a596ad0b82aa']\n\nseries_id = [s for s in series_id if s not in weird_series]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:11:32.390756Z","iopub.execute_input":"2023-10-17T23:11:32.39107Z","iopub.status.idle":"2023-10-17T23:11:32.395704Z","shell.execute_reply.started":"2023-10-17T23:11:32.391044Z","shell.execute_reply":"2023-10-17T23:11:32.394882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain = Parallel(n_jobs=6)(delayed(feat_eng_by_id)(i) for i in series_id)\ntrain = pd.concat(train, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:12:51.554493Z","iopub.execute_input":"2023-10-17T23:12:51.554986Z","iopub.status.idle":"2023-10-17T23:19:13.286493Z","shell.execute_reply.started":"2023-10-17T23:12:51.554951Z","shell.execute_reply":"2023-10-17T23:19:13.28495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# REDUCE train data by half\n# train = train.iloc[::36]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = ['series_id', 'step', 'timestamp']\n\nX, y = train.drop(columns=drop_cols+['awake']), train['awake']\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not DEV:\n    del train\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EnsembleAvgProba():\n    \n    def __init__(self, classifiers):\n        \n        self.classifiers = classifiers\n    \n    def fit(self,X,y):\n        \n        for classifier in self.classifiers:                \n            classifier.fit(X, y)\n            gc.collect()\n     \n    def predict_proba(self, X):\n        \n        probs = []\n        \n        for m in self.classifiers:\n            probs.append(m.predict_proba(X))\n        \n        probabilities = np.stack(probs)\n        p = np.mean(probabilities, axis=0)\n        \n        return p \n    \n    def predict(self, X):\n        \n        probs = []\n        \n        for m in self.classifiers:\n            probs.append(m.predict(X))\n        \n        probabilities = np.stack(probs)\n        p = np.mean(probabilities, axis=0)\n        \n        return p.round()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:26.922712Z","iopub.execute_input":"2023-10-17T00:09:26.923138Z","iopub.status.idle":"2023-10-17T00:09:26.934489Z","shell.execute_reply.started":"2023-10-17T00:09:26.923109Z","shell.execute_reply":"2023-10-17T00:09:26.932898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params1 = {    \n    'boosting_type':'gbdt',\n    'num_leaves':35,\n    'max_depth':6,\n    'learning_rate':0.03,\n    'n_estimators':850,\n    'subsample_for_bin':200000,\n    'min_child_weight':0.001,\n    'min_child_samples':20,\n    'subsample':0.9,\n#     'colsample_bytree':0.7,\n    'reg_alpha':0.05,\n    'reg_lambda':0.05,\n             }\nimport xgboost as xgb\n\n\n# Training classifier\n\nmodel = EnsembleAvgProba(classifiers=[\n                    lgb.LGBMClassifier(random_state=42, **lgb_params1),\n                    GradientBoostingClassifier(n_estimators=300,max_depth=5,min_samples_leaf=300,random_state=42),\n                    RandomForestClassifier(n_estimators=500, min_samples_leaf=300, random_state=42, n_jobs=-1),\n                    xgb.XGBClassifier(n_estimators=520,objective=\"binary:logistic\", learning_rate=0.02, max_depth=7, random_state=42)    ]\n                )","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:11:19.300918Z","iopub.execute_input":"2023-10-17T00:11:19.302642Z","iopub.status.idle":"2023-10-17T00:11:19.314464Z","shell.execute_reply.started":"2023-10-17T00:11:19.302587Z","shell.execute_reply":"2023-10-17T00:11:19.312741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:35.463307Z","iopub.execute_input":"2023-10-17T00:09:35.463857Z","iopub.status.idle":"2023-10-17T00:09:51.336323Z","shell.execute_reply.started":"2023-10-17T00:09:35.463817Z","shell.execute_reply":"2023-10-17T00:09:51.334622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = []\n\nfeat_imp = model.classifiers[0].booster_.feature_importance(importance_type='gain')\nfeat_imp = pd.Series(model.classifiers[0].feature_importances_, index=X.columns).sort_values()\nfeats.append(feat_imp)\n\nfor m in model.classifiers[1:]:\n    feat_imp = pd.Series(m.feature_importances_, index=X.columns).sort_values()\n    feats.append(feat_imp)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:51.339354Z","iopub.execute_input":"2023-10-17T00:09:51.339941Z","iopub.status.idle":"2023-10-17T00:09:51.352641Z","shell.execute_reply.started":"2023-10-17T00:09:51.339897Z","shell.execute_reply":"2023-10-17T00:09:51.351693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_imp = pd.Series(pd.concat(feats, axis=1).mean(axis=1), index=feats[0].index).sort_values()\nprint('Columns with poor contribution', feat_imp[feat_imp<0.001].index, sep='\\n')\nfig = px.bar(x=feat_imp, y=feat_imp.index, orientation='h')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:51.353998Z","iopub.execute_input":"2023-10-17T00:09:51.354594Z","iopub.status.idle":"2023-10-17T00:09:53.63724Z","shell.execute_reply.started":"2023-10-17T00:09:51.354544Z","shell.execute_reply":"2023-10-17T00:09:53.636267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_imp.sort_values().head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:53.64Z","iopub.execute_input":"2023-10-17T00:09:53.641162Z","iopub.status.idle":"2023-10-17T00:09:53.651981Z","shell.execute_reply.started":"2023-10-17T00:09:53.641119Z","shell.execute_reply":"2023-10-17T00:09:53.650349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del X, y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:53.653906Z","iopub.execute_input":"2023-10-17T00:09:53.654274Z","iopub.status.idle":"2023-10-17T00:09:54.158689Z","shell.execute_reply.started":"2023-10-17T00:09:53.654246Z","shell.execute_reply":"2023-10-17T00:09:54.156583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feat_eng2(df):\n    \n    df['series_id'] = df['series_id'].astype('category')\n    df['timestamp'] = pd.to_datetime(df['timestamp']).apply(lambda t: t.tz_localize(None))\n    df['hour'] = df[\"timestamp\"].dt.hour\n    \n    df.sort_values(['timestamp'], inplace=True)\n    df.set_index('timestamp', inplace=True)\n    \n    df['lids'] = np.maximum(0., df['enmo'] - 0.02)\n    df['lids'] = df['lids'].rolling(f'{120*5}s', center=True, min_periods=1).agg('sum')\n    df['lids'] = 100 / (df['lids'] + 1)\n    df['lids'] = df['lids'].rolling(f'{360*5}s', center=True, min_periods=1).agg('mean').astype(np.float32)\n    \n    df[\"enmo\"] = (df[\"enmo\"]*1000).astype(np.int16)\n    df[\"anglez\"] = df[\"anglez\"].astype(np.int16)\n    df[\"anglezdiffabs\"] = df[\"anglez\"].diff().abs().astype(np.float32)\n    \n    for col in ['enmo', 'anglez', 'anglezdiffabs']:\n        \n        # periods in seconds 1 5 12, 18, 24, 30, 60, 90, 120, 240, 360\n        periods = [60, 300, 720, 1080, 1800, 3600, 5400, 7200, 14400, 21600]\n        \n        for n in periods:\n            \n            rol_args = {'window':f'{n+5}s', 'min_periods':10, 'center':True}\n            \n            for agg in ['median', 'mean', 'max', 'min', 'var']:\n                df[f'{col}_{agg}_{n}'] = df[col].rolling(**rol_args).agg(agg).astype(np.float32).values\n                gc.collect()\n            \n            if n == max(periods):\n                df[f'{col}_mad_{n}'] = (df[col] - df[f'{col}_median_{n}']).abs().rolling(**rol_args).median().astype(np.float32)\n            \n            df[f'{col}_amplit_{n}'] = df[f'{col}_max_{n}']-df[f'{col}_min_{n}']\n            df[f'{col}_amplit_{n}_min'] = df[f'{col}_amplit_{n}'].rolling(**rol_args).min().astype(np.float32).values\n            \n#             if col in ['enmo', 'anglez']:\n            df[f'{col}_diff_{n}_max'] = df[f'{col}_max_{n}'].diff().abs().rolling(**rol_args).max().astype(np.float32)\n            df[f'{col}_diff_{n}_mean'] = df[f'{col}_max_{n}'].diff().abs().rolling(**rol_args).mean().astype(np.float32)\n\n    \n            gc.collect()\n    \n    df.reset_index(inplace=True)\n    df.dropna(inplace=True)\n#     df = df.iloc[::60]\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_events(idx, classifier, file='test_series.parquet') :\n    \n    test  = pd.read_parquet(f'/kaggle/input/child-mind-institute-detect-sleep-states/{file}',\n                    filters=[('series_id','=',idx)])\n    test = feat_eng2(test)\n    X_test = test.drop(columns=drop_cols)\n    test = test[drop_cols]\n\n    preds, probs = classifier.predict(X_test), classifier.predict_proba(X_test)[:, 1]\n    \n    test['prediction'] = preds\n    test['prediction'] = test['prediction'].rolling(360+1, center=True).median()\n    test['probability'] = probs\n    \n    test = test[test['prediction']!=2]\n    \n    test.loc[test['prediction']==0, 'probability'] = 1-test.loc[test['prediction']==0, 'probability']\n    test['score'] = test['probability'].rolling(60*12*5, center=True, min_periods=10).mean().bfill().ffill()\n\n    \n    test['pred_diff'] = test['prediction'].diff()\n    \n    test['event'] = test['pred_diff'].replace({1:'wakeup', -1:'onset', 0:np.nan})\n    \n    test_wakeup = test[test['event']=='wakeup'].groupby(test['timestamp'].dt.date).agg('first')\n    test_onset = test[test['event']=='onset'].groupby(test['timestamp'].dt.date).agg('last')\n    test = pd.concat([test_wakeup, test_onset], ignore_index=True).sort_values('timestamp')\n\n    return test","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:54.160155Z","iopub.execute_input":"2023-10-17T00:09:54.161419Z","iopub.status.idle":"2023-10-17T00:09:54.174499Z","shell.execute_reply.started":"2023-10-17T00:09:54.16137Z","shell.execute_reply":"2023-10-17T00:09:54.172718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_sub = ['series_id','step','event','score']\n\nseries_id  = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet', columns=['series_id'])\nseries_id = series_id.series_id.unique()\n\ntests = []\n\nfor idx in series_id: \n\n    test =  get_events(idx, model)\n    tests.append(test[cols_sub])","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:09:54.176655Z","iopub.execute_input":"2023-10-17T00:09:54.177605Z","iopub.status.idle":"2023-10-17T00:11:09.283686Z","shell.execute_reply.started":"2023-10-17T00:09:54.177554Z","shell.execute_reply":"2023-10-17T00:11:09.282692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat(tests, ignore_index=True).reset_index(names='row_id')\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-10-17T00:11:09.284871Z","iopub.execute_input":"2023-10-17T00:11:09.286083Z","iopub.status.idle":"2023-10-17T00:11:09.308224Z","shell.execute_reply.started":"2023-10-17T00:11:09.286045Z","shell.execute_reply":"2023-10-17T00:11:09.306865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.layers import LSTM,GRU,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling2D,Conv1D,SpatialDropout1D,MaxPooling1D\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n\n# maxlen =37\n# trunc_type='post'\n\n# oov_tok = \"<OOV>\"\n# vocab_size = 300","metadata":{"execution":{"iopub.status.busy":"2023-10-10T07:42:59.655105Z","iopub.status.idle":"2023-10-10T07:42:59.655712Z","shell.execute_reply.started":"2023-10-10T07:42:59.655536Z","shell.execute_reply":"2023-10-10T07:42:59.655555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class TransformerBlock(layers.Layer):\n#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n#         super().__init__()\n#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n#         self.ffn = keras.Sequential(\n#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n#         )\n#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        \n#         self.dropout1 = layers.Dropout(rate)\n#         self.dropout2 = layers.Dropout(rate)\n\n#     def call(self, inputs, training):\n#         attn_output = self.att(inputs, inputs)\n#         attn_output = self.dropout1(attn_output, training=training)\n#         out1 = self.layernorm1(inputs + attn_output)\n#         ffn_output = self.ffn(out1)\n#         ffn_output = self.dropout2(ffn_output, training=training)\n#         return self.layernorm2(out1 + ffn_output)\n# class TokenAndPositionEmbedding(layers.Layer):\n#     def __init__(self, maxlen, vocab_size, embed_dim):\n#         super().__init__()\n#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n#     def call(self, x):\n#         maxlen = tf.shape(x)[-1]\n#         positions = tf.range(start=0, limit=maxlen, delta=1)\n#         positions = self.pos_emb(positions)\n#         x = self.token_emb(x)\n#         return x + positions\n# embed_dim = 32  # Embedding size for each token\n# num_heads = 2  # Number of attention heads\n# ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\n# inputs = layers.Input(shape=(maxlen,))\n# embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n# x = embedding_layer(inputs)\n# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n# x = transformer_block(x)\n# x=layers.Bidirectional(LSTM(64, return_sequences=True))(x)\n# x = layers.GlobalAveragePooling1D()(x)\n# x = layers.Dropout(0.1)(x)\n# x = layers.Dense(20, activation=\"relu\")(x)\n# x = layers.Dropout(0.1)(x)\n# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\n# model = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T07:42:59.656818Z","iopub.status.idle":"2023-10-10T07:42:59.657409Z","shell.execute_reply.started":"2023-10-10T07:42:59.657232Z","shell.execute_reply":"2023-10-10T07:42:59.657255Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary()\n# model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.RMSprop(1e-3),metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-10-16T00:04:10.521094Z","iopub.execute_input":"2023-10-16T00:04:10.521447Z","iopub.status.idle":"2023-10-16T00:04:10.551587Z","shell.execute_reply.started":"2023-10-16T00:04:10.521409Z","shell.execute_reply":"2023-10-16T00:04:10.550447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\n# pocket = EarlyStopping(monitor='val_exact_matched_accuracy', min_delta=0.001,\n#                        patience=10, verbose=1, mode='max', \n#                        restore_best_weights = True)\n# reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n#                               mode='auto',verbose=1)\n# history=model.fit(xtrain,ytrain,epochs=50,batch_size=256,validation_data=(xtest,ytest),callbacks=[pocket,reduce_lr])\n# # history = pd.DataFrame(history.history)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T07:42:59.661593Z","iopub.status.idle":"2023-10-10T07:42:59.662156Z","shell.execute_reply.started":"2023-10-10T07:42:59.661861Z","shell.execute_reply":"2023-10-10T07:42:59.661886Z"},"trusted":true},"execution_count":null,"outputs":[]}]}